
\title{Benchmarking Spark}

\author{Gregor von Laszewski}
\affiliation{%
  \institution{Indiana University}
  \streetaddress{Smith Research Center}
  \city{Bloomington}
  \state{IN}
  \postcode{47408}
}
\email{laszewski@gmail.com}

\author{Karan Kamatgi}
\affiliation{%
  \institution{Indiana University}
  \streetaddress{School of Informatics, Computing, and Engineering}
  \city{Bloomington}
  \state{IN}
  \postcode{47404}
}
\email{krkamatg@iu.edu}

\author{Karan Kotabagi}
\affiliation{%
  \institution{Indiana University}
  \streetaddress{School of Informatics, Computing, and Engineering}
  \city{Bloomington}
  \state{IN}
  \postcode{47404}
}
\email{kkotabag@iu.edu}

\author{Manoj Joshi}
\affiliation{%
  \institution{Indiana University}
  \streetaddress{School of Informatics, Computing, and Engineering}
  \city{Bloomington}
  \state{IN}
  \postcode{47404}
}
\email{manjoshi@iu.edu}

\author{Ramyashree DG}
\affiliation{%
  \institution{Indiana University}
  \streetaddress{School of Informatics, Computing, and Engineering}
  \city{Bloomington}
  \state{IN}
  \postcode{47404}
}
\email{rdasego@iu.edu}

% The default list of authors is too long for headers}
\renewcommand{\shortauthors}{G. v. Laszewski, Karan Kamatgi, 
  Karan Kotabagi, Manoj Joshi, Ramyashree DG}
  
\begin{abstract}
Write abstract here 
\end{abstract}

\keywords{Spark, Raspberry Pi, Futuresystems, Docker, Hadoop 
V/S Spark, Spark Cluster}


\maketitle

\section{Introduction}

Bigdata is ruling today's business and with technology boom and digitalization, 
large amount data As we all know big data has gained wide range of importance in
various aspects. Big data, as a word refers to huge amount of data which are 
both organized and disorganized. However, the main important thing here is not
the amount of data, instead how this data is used by organizations. Big data,
plays a vital role in having clear insights about the growth of business.
Big data has determined its importance in various factors such as deducing the 
root cause of failed features, customizing coupon generation based on the 
customer’s buying habits, risk analysis of the business organization, 
determining irregular activities through data analysis which can affect the 
organization’s growth. Big data has marked its importance in various 
organizations across practically every industry such as banking, education, 
government, health care, manufacturing and retail. To be specific  Big data is 
used in various fields such as understanding and targeting customers, optimizing
business process, personal qualification and performance optimization, 
improving health care and public health, Improvising business in sports, 
machine and device performance, improving security and law enforcement,  
financial trading and also in improving science and research. Thus, all these 
various applications of big data are not possible from the raw form of the 
data, it is required to process and analyze this data
\cite{hid-sp18-406-hadoop-abstract}.

\section{Introduction to Hadoop}

\subsection{Why do we need Hadoop?}

Data is generated continuously and rigorously from each and every application in
almost all the platforms such as social media, mobile platform and many more. 
The solutions to handle this data should be very quick and also should consider
business cost required for this analysis.The problem with early storage tools 
such as RDBMS is that it is unable to process the semi-structured and 
unstructured data such as   text files, videos, audios, clickstream data etc and
it is suitable only for structured data such as banking transaction, location 
information, etc. These both forms of data are completely different in ways 
required for processing the data. The main flaw in RDBMS is that it is unable to
scale vertically with large number of CPU and other storages. It is a main 
problem if the main server is down, this created the need for distributed system
which can be robust and handle scalability. Hadoop is trusted for its effective 
management of largely sized data which is both structred and unstructured in 
different forms such as XML, JSON, and also text at high fault-error tolerance. 
It is noticed that with clusters of many servers in the field of horizontal 
scalability, Hadoop has marked its significance by providing faster result rates
from Big Data and also the unstructured data as Hadoop's architecture is based 
on the open source foundation. Other challenges faced with the big data where 
keeping the huge amount of data secure, analyzing the data without knowing the 
type of the data, dealing with data which has poor quality and is inconsistent 
and incomplete. Also, finding powerful algorithms to find patterns and insights
was not an easy task. On the other hand, the organizations and enterprises 
realized that the amount of information which is used for analyzing is less and 
majority of the data is getting wasted. The main reason behind this is that the 
organizations lacks power and strong tools to analyze such huge amount of data. 
Due to these limitations of processesing data, huge amount of valuable data was 
termed as unwanted and discarded. It is useful and important to collect and 
keep all the data in a safe storage, which the business organizations realized
~\cite{hid-sp18-406-hadoop-intro1}.

Hadoop emerged as a strongest  tool to deal with the above challenges of the big
data. Now, almost all organizations use big data analysis to improve the 
functionalities in each and every business unit. The various kinds of business 
units include research, design, development, marketing, advertising, sales  and 
customer handling. Sharing such huge amount of data across different platforms  
is also challenging,  in such scenarios hadoop is used to create a pond. Hadoop 
in this way represents repository of various sources of data which may be from 
intrinsic or extrinsic sources. Hadoop is a set of open source programs and 
procedures which can be used for big data operations. It is an open source 
framework from Apache which is comprised of Java based programming framework 
that can be used for data storage and processing of the data-sets clusters on 
commodity hardware. The basic idea behind hadoop is that to make the computation
of the data faster. Hadoop MapReduce is composed of shared and integrated 
foundation where developers can include additional tools and enhance the 
framework. Additionally, scalability is the core of the hadoop system. The novel
approach which emerged hadoop is that by storing the all types of data 
available, focus on organizing and analyzing the data in new interesting ways
~\cite{hid-sp18-406-hadoop-intro2}.

The growth of Hadoop has marked its significance in changing the perception of 
handling Big Data, specifically for the unstructured data. Using Hadoop we can 
enable excess data streamlining for any distributed processing system across 
clusters. Hadoop system helps scaling up from single server to a large number 
of servers, and also assuring local computation and storage space on every 
distributed server. Hadoop does not depend on hardware to provide 
high-availability; instead it makes its software efficient by building robust 
libraries. These libraries will handle the breakdown at the application layer 
and thus helping the service to be efficient along with the cluster of computers
~\cite{hid-sp18-406-hadoop-intro3}.

Hadoop as framework has established credibility for various factors. It allows 
the users to easily frame test cases on distributed systems. These test cases 
are efficiently deployed across various machines in turn and it also utilizes 
the parallelism of the CPU cores. Another interesting feature of hadoop is that 
it does not rely on hardware to take care of the fault tolerance, instead hadoop
is designed to detect the failures and handle them at the application layer 
itself. Additionally, servers configuration, addition and removal is independent
of the hadoop operation and these servers can be added and detached to the 
clusters dynamically during the operations. One of the major advantage and main 
reason for hadoop to be successful is that being an open source framework it is 
suitable for all the frameworks as it is Java based
~\cite{hid-sp18-406-hadoop-intro4}.

In simple words to state, hadoop provides efficient ways of storing enormous 
data sets by distributing it to various clusters and then execute distributed 
analysis application in each cluster. The other main important feature about 
hadoop is that, it is a framework which is modular and is compatible to 
integrate with any of the required modules. Thus, it allows swapping of the 
components according the need of software tool which results in flexible 
architecture, which makes hadoop robust and efficient. To the business 
organizations, the flexibility feature of hadoop is most advantageous, as it 
allows the business users to add or modify their data storage and analysis  
according to the business needs and suitable software.


\subsection{Overview of Hadoop Modules}
Apache's hadoop framework in composed of the four main modules: 
a)Hadoop Common: which hold the libraries and utilities required by the Hadoop 
modules.
b)Hadoop Distributed File System: which manages to store data on commodity 
machines, which is helpful in providing efficient bandwidth across the cluster.
c) Hadoop Yarn: this is basically a resource management platform which will be 
responsible managing compute resources in clusters and using them for scheduling
of user's applications.
d) Hadoop MapReduce: this is the programming module required for large scale 
data processing.
The main principle behind the above mentioned modules of Hadoop is that the 
common scenarios of  hardware failures is handled by software in the framework
~\cite{hid-sp18-406-hadoop-intro5}.

\subsection{Importance of Yarn}
Yet Another Resource Negotiator (YARN) is one of the most important components 
of Apache hadoop as it takes care of managing resources and scheduling tasks and
thus forming the clustering platform. YARN is responsible for setting up the 
global and application specific components which are required for resource 
management. For any particular application, the required and suitable resources 
are allocated by the YARN. Initially, the application submission client will 
submit an application to the resource manager of YARN. Further, YARN takes the 
responsibility of scheduling application so that tasks are prioritized and big 
data analytics of the system can be managed. This results as the greater step 
of system architecture for collecting the data and sorting them and further 
conducting requirement specific queries such as data retrieval. Such type of 
information retrieval has found very great business values because the 
organizations can use data analyzing platforms for supply chain maintenance, 
product documentation, and various service operations such as maintaining 
customer information and also for maintaining automated processes of business
~\cite{hid-sp18-406-hadoop-intro6}.

Being an essential part of core hadoop projects, YARN is capable of allowing 
multiple data processing engines like interactive SQL, real-time streaming, data
science and batch processing. Likewise YARN is growing to be a mandatory 
foundation for new generation hadoop tools and has become an important component
which is required for realizing modern data architecture. Additionally, YARN is 
extending the capabilities of hadoop by providing support for the new 
technologies which will be found within the data center. This helps in various 
factors such as cost effective solutions, linear scalability for storing and 
processing. Further, YARN is still in progress for improvising factors 
specifically for the new engines which are emerging to interact with data 
storage and analysis. Thus, YARN is one amongst the reliable architectural 
center of hadoop  and definitely  the most important foundational component
~\cite{hid-sp18-406-hadoop-intro7}.

\subsection{Advantages of Hadoop}
1. Scalable : 
Hadoop is specifically designed to have a very flat scalability structure. 
Once After a Hadoop program is written and is functioning on ten nodes, very 
little work is required for that same program to run on a much larger setup. 
The underlying Hadoop platform will manage the data and hardware resources and 
provide dependable performance growth (but proportionate to the number of 
machines available). Also, it is a highly scalable storage platform, because it 
can store and distribute large data sets across hundreds of inexpensive servers 
that operate in parallel. Hadoop enables businesses to run applications on 
thousands of nodes involving thousands of terabytes of data
~\cite{hid-sp18-406-hadoop-intro8}.

2.Advanced data analysis can be done in-house : 
With Hadoop environment its capable to work with large data sets and customize 
the outcome without having to outsource the task. Keeping operations in-house 
helps organizations be more agile, while also avoiding the ongoing operational 
expense of outsourcing.

3. Organizations can fully leverage their data : 
With Hadoop systems, organizations can take full advantage of all their data – 
structured and unstructured, real-time and historical.Earlier with traditional 
legacy systems, all the available data and inputs were not used to do analysis 
to support business activity. Leveraging adds more value to the data itself and 
improves the return on investment (ROI) for the legacy systems.

4. Flexible architecture : 
Some of the tasks that Hadoop is being used for today were formerly run by 
expensive computer systems. Hadoop commonly runs on commodity hardware. Because 
of big data standard, Hadoop is supported by a large and competitive solution 
provider community, which protects customers from vendor lock-in
~\cite{hid-sp18-406-hadoop-intro9}.

5. Cost Effective : 
Hadoop systems provides cost effective storage solution for businesses. The 
problem with traditionalsystems is that it is extremely cost prohibitive to 
process such massive volumes of data. To reduce costs, many companies in the 
past would have had to down-sample data and classify it based on certain 
assumptions as to which data was the most valuable. The raw data would be 
deleted, as it would be too cost-prohibitive to keep. While this approach may 
have worked in the short term providing very cost-effective solution for 
expanding datasets. It is designed to scale-out architecture that can affordably
store all company's data for use sometime later. This saves a lot of cost and 
improves the storage capability tremendously.

6. Enhances Speed : 
Hadoop's unique storage method is based on a distributed file system that 
basically maps data wherever it is located on a cluster. The tools for data 
processing are often on the same servers where the data is located, resulting 
in much faster data processing. If you’re dealing with large volumes of 
unstructured data, it can efficiently process terabytes of data in just minutes
~\cite{hid-sp18-406-hadoop-intro10}.

7. Great Data Reliability : 
Data reliability is one aspect that no organization wants to compromise on. 
Hadoop provides complete confidence and reliability; in a scenario where data 
loss happens on a regular basis, HDFS helps you solve the issue. It stores and 
delivers all data without compromising on any aspect, at the same time keeping 
costs down. Whether you are a start-up, a government organization, or an 
internet giant, Hadoop has proved its mettle when it comes to strong data 
reliability in a variety of production applications at full scale.

8. Comprehensive Authentication and Security : 
All businesses are looking for software that makes their work safe, secure and 
authenticated. When it comes to authentication and security, Hadoop provides an 
advantage over other software. Its HBase security, along with HDFS and 
MapReduce, allows only approved users to operate on secured data, thereby 
securing an entire system from unwanted or illegal access. Security is the top 
priority of every organization. Any unlawful access to data is sure to harm 
business dealings and operations~\cite{hid-sp18-406-hadoop-intro11}.

9. Flexible with Range of data sources : 
Hadoop enables businesses to easily access new data sources and tap into 
different types of data. The data collected from various sources will be of 
structured or unstructured form. This means organizations can use Hadoop to 
derive valuable business insights from data sources such as social media, 
clickstream data,  or email conversations.A lot of time would need to be 
allotted to convert all the collected data into single format. Hadoop saves 
this time as it can derive valuable data from any form of data. It also has a 
variety of functions such as data warehousing, fraud detection, market campaign 
analysis etc ~\cite{hid-sp18-406-hadoop-intro12}.

10. Resilient to failure : 
A key advantage of using Hadoop is its fault tolerance. When data is sent to an 
individual node, that data is also replicated to other nodes in the cluster, 
which means that in the event of failure, there is another copy available for 
use ~\cite{hid-sp18-406-hadoop-intro13}.

\section{Introduction to Apache Spark}
Apache Spark is a cluster computing framework developed 
at the University of California, Berkeley. It is maintained as an open software 
by the Apache Software Foundation. It provides an interface for programming with 
data parallelization and also fault tolerance. Spark's architecture is based on 
Resilient Distributed Dataset (RDD) which is a read-only set of data items split
over a cluster of machines. In short, instead of the code getting the data for 
computation, it goes to the location where the data is present
\cite{hid-sp18-408-Spark}. 

\subsection{RDD}
RDD is a fundamental data structure of Spark. It is an immutable distributed 
collection of objects wherein each dataset is divided into logical partitions. 
Computation is performed on different nodes of the cluster. RDD's can have any 
type of objects like Java, Scala, Python objects or even user-defined class 
objects. An RDD is a read-only, collection of records that are partitioned. 
RDD's can be created through operations on data on storage or using other RDD's.
RDD's are a collection of elements which are fault-tolerant and operate 
parallely. RDD's can be created in two ways - parallelizing an existing 
collection in the driver program, or referencing a dataset in an external 
storage system \cite{hid-sp18-408-Spark-RDD}.

Spark and its RDD's were developed to overcome the limitations of Map-Reduce in 
Hadoop which forces a linear dataflow on distributed programs. Map-Reduce 
programs read the data from the disk, map some function across the data, do the 
reduce operation on the results of the map and finally store the results from 
reduce onto a disk. On the other hand, Spark's RDD function by giving a working 
set for distributed programs and offer distributed shared memory in a restricted
way. 

Spark provides platform for implementation of iterative algorithms and 
interactive data analysis. Iterative algorithms visit the data set multiple 
times and interactive data analysis involves repeated querying of database. 
The latency of such applications can be reduced greatly when compared to a 
Map-Reduce implementation of the task. 

Apache Spark requires  a manger to handle the cluster load and also needs a 
distributed storage system. For cluster management, Spark supports standalone 
cluster, Hadoop Yarn and Apache Mesos. To provide distributed storage, Spark 
can be integrated with a variety of frameworks like Hadoop Distributed File 
System (HDFS), MapR File System (MapR-FS), Cassandra, Kudu. Also, a custom 
solution can be implemented and integrated into Spark. 

\subsection{Spark Core}
Spark has a core component called the Spark Core that provides distributed task 
dispatching, Input-Output functionalities and scheduling exposed through an API 
centered on RDD abstraction. A driver program invokes parallel operations on an 
RDD by passing a function to Apache Spark which then schedules the function's 
execution in parallel on the whole cluster. Operations such as joins, take RDD's 
as an input and produce new RDD's. RDD's are immutable and fault-tolerance is 
achieved by keeping a track of the lineage of each RDD so that if there is a 
data loss, it can be reconstructed. RDD's support any type of Python, Scala or 
Java objects. Along with the RDD style of programming, Spark provides two 
restricted forms of shared variables - broadcast variables and accumulators. 
Broadcast variables reference read-only data and makes it available on all 
nodes. Accumulators can be used to program reduce part in an imperative style. 

\subsection{Spark SQL}
Spark SQL is a component that is built on top of Spark Core that provides a data 
abstraction called DataFrame. DataFrame provides support for structured and 
semi-structured data. To support various languages like Scala, Java or Python, 
Spark SQL provides domain-specific language (DSL). Spark streaming makes use of 
Spark Core's fast scheduling capacity to perform streaming analytics. It takes
in data in mini-batches and performs RDD transformations on those mini-batches 
of data. This enables the same set of application code written for batch 
analytics to be used for streaming analytics as well enabling easy 
implementation of lambda architecture. Spark Streaming has support to consume 
from Kafka, Twitter, Flume, Kinesis and TCP/IP sockets.

\subsection{Spark MLlib}
Spark MLlib (Machine Learning Library) is a distributed machine learning 
framework that is built on top of Spark Core. Many common machine learning and 
statistical algorithms have been implemented in MLlib which facilitates large 
scale machine learning pipelines. Some of the supervised and unsupervised 
Machine Learning algorithms implemented are Support Vector Machine, Logistic 
Regression, Linear Regression, Decision Trees, Naive Bayes, Latent Dirichlect 
Allocation. 


\subsection{Spark GraphX}
GraphX is a distributed graph processing framework built on top of Apache Spark. 
Since RDD's are immutable, graphs are also immutable and hence GraphX is 
unsuitable for graphs that need to be updated. GraphX provides two seperate 
application programming interface for implementing parallel algorithms - a 
Pregel abstraction and a MapReduce style API \cite{hid-sp18-408-Spark}.


\section{Applications of Spark}
The applications of Apache Spark range from Streaming Data, Machine Learning, 
Interactive Analysis and Fog Computing.

\subsection{Spark Streaming Data}
Spark's key use case is its ability to process continuously 
streaming data. Spark Streaming unifies different types of data processing 
capabilities allowing developers to use just a single framework to handle the 
processing needs. The most common ways that Spark Streaming is used today are 
Streaming ETL, Data Enrichment, Trigger Event Detection and Complex Session 
Analysis. Streaming ETL - Traditional ETL tools used for batch processing, read 
the data, convert it to a database compatible format and finally write it to the
database. In Streaming ETL, data is cleaned and aggregated continually before it 
is pushed into data stores. Data Enrichment - This streaming capability enriches 
live data by combining it with static data. This allows user to conduct more 
real-time data analysis. Trigger event detection - Spark Streaming allows user 
to detect and respond quickly to unusual behaviors in real-time. Complex Session
Analysis - Using the Spark Streaming feature, events with respect to live 
sessions such as user activity in a website or an application. These session 
informations can be used continuously to update the machine learning models. 
Machine Learning: Apache Spark comes integrated with a framework for performing 
advanced analytics. It helps users run repeated queries on data sets which 
amounts to processing machine learning algorithms. Spark supports machine 
learning through its framework called MLlib which can perform clustering, 
classification among many other algorithms. 


\subsection{Spark Interactive Analysis}
One of the most notable features of Spark is its ability 
to provide interactive analytics. Unlike Hadoop, Spark performs exploratory 
queries without sampling. By integrating Spark with visualization tools, complex
data sets can be processed visualized in an interactive way.
Fog Computing: Fog computing decentralizes data processing and storage instead 
of performing computation on the edge of the network . However, this creates a 
lot of complexities for processing decentralized data since it requires low 
latency along with massive parallel processing for running machine learning 
algorithms.  But with Spark Streaming, real-time querying tool (Shark), MLlib 
and GraphX, Spark qualifies as a fog computing solution
\cite{hid-sp18-408-Spark-Use-Case}.


\section{Raspberry Pi}
Raspberry Pi is a series of mini single-board computers. 
It was developed by Raspberry Pi Foundation to promote teaching of basic 
computer science. Till date, various Raspberry Pi's have been released with a 
system on chip and ARM compatible CPU along with on-chip GPU.   The processor 
speed ranges from 700 MHz to 1.4 GHz and on-board memory ranges from 256 MB to 
1 GB RAM. SD cards can be used to store operating system and program memory. The
boards have USB ports ranging from one to four ports. For video output, HDMI is 
supported and 3.5 mm phono jack supports audio output. The Raspberry Pi 
Foundation provides Raspbian, a Debian-based Linux distribution along with 
Ubuntu, Windows 10 IoT Core and specialized media center distributions. It 
supports Python and Scratch as the main programming language and also supports 
many other languages
\cite{hid-sp18-408-Raspberry-Pi}.


\section{Difference between Spark and MapReduce} 
Spark keeps data in memory whereas MapReduce keeps shuffling things. MapReduce 
takes a long time to write things to disk and also to read them back. This makes
MapReduce slow. For SQL queries, a chain of MapReduce operations are usually 
required and since MapReduce keeps shuffling things, it required a lot of 
Input-Output activity. On the other hand, when SQL queries are run on Spark, it 
executes faster since it has data in memory and requires less Input-Output 
operations. Spark has a Map and a Reduce function like MapReduce, but it also 
has richer features such as Filter, Join and Group-by. Hence development in 
Spark is easier and is flexible. Spark provides a lot of instructions at a 
higher level of abstraction than what MapReduce provides
\cite{hid-sp18-408-Difference}.


\begin{acks}

  The authors would like to thank Dr.~Gregor~von~Laszewski 
  for his support and suggestions to write this paper.
  
\end{acks}

\bibliographystyle{ACM-Reference-Format}
\bibliography{report}